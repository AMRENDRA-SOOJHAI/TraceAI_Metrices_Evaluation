ğŸ“Š LLM Evaluation Metrics

This document explains the most commonly used evaluation metrics for Large Language Models (LLMs). These metrics help measure fluency, accuracy, similarity, coverage, and semantic understanding of generated text.

ğŸ”¹ 1. Perplexity
ğŸ“Œ What is Perplexity?

Perplexity measures how confused a language model is when predicting text.

In simple terms:
Perplexity tells you how many choices, on average, the model thinks it has for the next word.

ğŸ” Interpretation

Low Perplexity â†’ Model is confident & predictable

High Perplexity â†’ Model is uncertain & confused

Perplexity Value	Interpretation
1 â€“ 10	Excellent language modeling
10 â€“ 50	Reasonable fluency
50 â€“ 100+	Poor prediction
ğŸ§® Formula
Perplexity
=
exp
â¡
(
âˆ’
1
ğ‘
âˆ‘
ğ‘–
=
1
ğ‘
log
â¡
ğ‘ƒ
(
ğ‘¤
ğ‘–
)
)
Perplexity=exp(âˆ’
N
1
	â€‹

i=1
âˆ‘
N
	â€‹

logP(w
i
	â€‹

))

Where:

N = Total number of tokens

w_i = i-th token

P(w_i) = Probability assigned to token w_i

âœ… Key Insight

Perplexity is the exponential of the average negative log-probability assigned to the correct next tokens.

â¡ï¸ Lower perplexity = better language understanding

ğŸ”¹ 2. BLEU (Bilingual Evaluation Understudy)
ğŸ“Œ What is BLEU?

BLEU measures how similar a generated text is to a reference (correct) text, based on n-gram overlap.

In simple terms:
BLEU checks how much the modelâ€™s output looks like a known good answer.

ğŸ§  How BLEU Works

BLEU evaluates n-gram precision:

Unigram â†’ Single words

Bigram â†’ 2-word sequences

Trigram â†’ 3-word sequences

ğŸ“Š BLEU Score Interpretation
BLEU Score	Meaning
0.7 â€“ 1.0	Very close to reference
0.4 â€“ 0.7	Reasonably similar
0.2 â€“ 0.4	Weak similarity
< 0.2	Poor match
ğŸ§® BLEU Formula
BLEU
=
ğµ
ğ‘ƒ
â‹…
exp
â¡
(
âˆ‘
ğ‘›
=
1
ğ‘
ğ‘¤
ğ‘›
log
â¡
ğ‘
ğ‘›
)
BLEU=BPâ‹…exp(
n=1
âˆ‘
N
	â€‹

w
n
	â€‹

logp
n
	â€‹

)

Where:

p_n = n-gram precision

w_n = n-gram weight (usually uniform)

BP = Brevity Penalty

âš ï¸ Notes

BLEU is precision-focused

Penalizes overly short outputs using Brevity Penalty

Less effective for paraphrasing or creative text

ğŸ”¹ 3. ROUGE (Recall-Oriented Understudy for Gisting Evaluation)
ğŸ“Œ What is ROUGE?

ROUGE measures overlap between generated text and reference text, with a focus on recall.

In GenAI tasks, missing important information is worse than adding extra words â€” ROUGE captures this.

ğŸ” Types of ROUGE
ğŸ”¸ ROUGE-N (n-gram overlap)

Measures how many n-grams from the reference appear in the generated text.

Formula (Recall):

ROUGE-N
=
Matching n-grams
Total n-grams in reference
ROUGE-N=
Total n-grams in reference
Matching n-grams
	â€‹

ğŸ”¸ ROUGE-L (Longest Common Subsequence)

Measures the longest common subsequence (LCS)

Captures sentence structure & fluency

ğŸ”¸ ROUGE-S (Skip-Bigram)

Matches word pairs with gaps

Useful for flexible phrasing (less common)

ğŸ“Š ROUGE Metrics
Metric	Meaning
Precision	How much generated text is relevant
Recall	How much reference text is covered
F1 Score	Balance between precision & recall
ğŸ”¹ 4. BERTScore
ğŸ“Œ What is BERTScore?

BERTScore measures semantic similarity using contextual embeddings from pretrained transformer models (e.g., BERT, RoBERTa).

ğŸš€ Why BERTScore?

Unlike BLEU or ROUGE, BERTScore:

âœ… Does not rely on exact word overlap

âœ… Understands meaning and paraphrases

âœ… Works better for modern LLM outputs

ğŸ§  How It Works

Encode tokens using contextual embeddings

Match generated tokens with reference tokens

Compute similarity using cosine similarity

ğŸ“Š BERTScore Outputs

Precision â€“ Semantic relevance of generated text

Recall â€“ Semantic coverage of reference text

F1 Score â€“ Overall semantic similarity

âœ¨ Two sentences with the same meaning but different words still achieve a high BERTScore

ğŸ§  Summary Comparison
Metric	Focus	Best For
Perplexity	Confidence	Language modeling
BLEU	Precision	Translation
ROUGE	Recall	Summarization
BERTScore	Semantics	Modern GenAI tasks
